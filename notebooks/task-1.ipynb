{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = 2\n",
    "c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(90)\n",
    "M = tf.Variable(tf.random.normal((a, b, c), 5, 2), name='M')\n",
    "p = tf.Variable(tf.random.uniform((), 0, 1, name='p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation using accumulative functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        reshaped_M = tf.reshape(M, [a*b, -1])  # dropping one dimension to ease the operations on the last dimension\n",
    "        N = tf.reshape(tf.map_fn(lambda row: tf.scan(lambda x, y: x*p+y, row), reshaped_M), M.shape)\n",
    "print('dN/dM:')\n",
    "print(tape.gradient(N, M))\n",
    "print('dN/dp:', tape.gradient(N, p).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same, but implemented with numpy - for verification purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_accumulation = np.frompyfunc(lambda x, y: y + p.numpy()*x, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_numpy = memory_accumulation.accumulate(M.numpy(), axis=2, dtype=object).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(N.numpy(), N_numpy)  # exact comparison doesn't work for numerical reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative - utilizng matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution above, even though effective, lacks some elegance. We could therefore represent it as matrix multiplication. The crucial observation is that by going along the last axis, we can develop a multinomial dependence on `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        powers_of_p = tf.linalg.band_part(p**tf.math.cumsum(tf.linalg.set_diag(tf.linalg.band_part(tf.ones((c, c)), 0, -1), tf.zeros((c))), 1), 0, -1)\n",
    "        N = tf.matmul(M, powers_of_p)  # we utilize broadcasting here\n",
    "print('dN/dM:')\n",
    "print(tape.gradient(N, M))\n",
    "print('dN/dp:', tape.gradient(N, p).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(N.numpy(), N_numpy)  # exact comparison doesn't work for numerical reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        reshaped_M = tf.reshape(M, [a*b, -1])\n",
    "        N = tf.reshape(tf.map_fn(lambda row: tf.scan(lambda x, y: x*p+y, row), reshaped_M), M.shape)\n",
    "tape.gradient(N, M)\n",
    "tape.gradient(N, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        powers_of_p = tf.linalg.band_part(p**tf.math.cumsum(tf.linalg.set_diag(tf.linalg.band_part(tf.ones((c, c)), 0, -1), tf.zeros((c))), 1), 0, -1)\n",
    "        N = tf.matmul(M, powers_of_p)\n",
    "tape.gradient(N, M)\n",
    "tape.gradient(N, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the latter solution (matrix multiplication) on the laptop I was running this code is ~15x faster and, additionally, doesn't raise warnings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
